import torch
import torchvision
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler
import matplotlib.pyplot as plt
import numpy as np
import os
from tqdm import tqdm
import time
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import json
from PIL import Image
import random
import sys

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Using device: {device}")
print(f"âœ… PyTorch version: {torch.__version__}")
print(f"âœ… CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ… GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
BATCH_SIZE = 32
NUM_EPOCHS = 40  # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
LEARNING_RATE = 0.0001  # Ğ‘Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ learning rate Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
NUM_WORKERS = 0  # 0 Ğ´Ğ»Ñ Windows
PATIENCE = 7  # Early stopping patience
MIN_IMAGES_PER_CLASS = 50

# ĞŸÑƒÑ‚Ğ¸
DATASET_PATH = "data/plantvillage_dataset"
MODEL_SAVE_PATH = "models/plant_disease_resnet50_final.pth"
BEST_MODEL_PATH = "models/plant_disease_resnet50_best.pth"
RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs("models", exist_ok=True)

print(f"\nğŸ“‚ Dataset path: {DATASET_PATH}")
print(f"ğŸ’¾ Best model save path: {BEST_MODEL_PATH}")

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
if not os.path.exists(DATASET_PATH):
    print(f"âŒ Dataset not found at: {DATASET_PATH}")
    print("Please ensure you copied the dataset manually to the correct location!")
    exit()

# ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ²
class_names = [d for d in os.listdir(DATASET_PATH)
               if os.path.isdir(os.path.join(DATASET_PATH, d)) and not d.startswith('.')]
class_names.sort()
NUM_CLASSES = len(class_names)

print(f"\nğŸ¯ Dataset Classes ({NUM_CLASSES} total):")
for i, class_name in enumerate(class_names, 1):
    class_path = os.path.join(DATASET_PATH, class_name)
    num_images = len([f for f in os.listdir(class_path)
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
    print(f"   {i}. {class_name.replace('_-_', ' - ')}: {num_images:,} images")

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹
print(f"\nğŸ”„ Setting up data transformations...")

# ĞÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸
train_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(p=0.6),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.RandomRotation(25),
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),
    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15), shear=10),
    transforms.RandomGrayscale(p=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ°
val_test_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
print("\nğŸ“¦ Loading dataset...")
full_dataset = datasets.ImageFolder(
    root=DATASET_PATH,
    transform=train_transforms
)

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ²
print("\nğŸ” Verifying class names match dataset structure...")
dataset_class_names = full_dataset.classes
for i, (expected, actual) in enumerate(zip(class_names, dataset_class_names)):
    print(f"   Class {i + 1}: Expected '{expected}', Actual '{actual}'")

# Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: 70% train, 15% val, 15% test
train_size = int(0.7 * len(full_dataset))
val_size = int(0.15 * len(full_dataset))
test_size = len(full_dataset) - train_size - val_size

train_dataset, val_test_dataset = random_split(full_dataset, [train_size, len(full_dataset) - train_size])
val_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])

# ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ°
val_dataset.dataset.transform = val_test_transforms
test_dataset.dataset.transform = val_test_transforms

print(f"\nğŸ“Š Dataset split:")
print(f"   Training:   {len(train_dataset):,} images ({len(train_dataset) / len(full_dataset):.1%})")
print(f"   Validation: {len(val_dataset):,} images ({len(val_dataset) / len(full_dataset):.1%})")
print(f"   Test:       {len(test_dataset):,} images ({len(test_dataset) / len(full_dataset):.1%})")

# Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ²
print("\nâš–ï¸ Setting up class balancing...")

# ĞŸĞ¾Ğ´ÑÑ‡ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸
class_counts = torch.zeros(NUM_CLASSES)
for _, label in train_dataset:
    class_counts[label] += 1

# Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°
class_weights = 1. / class_counts
class_weights = class_weights / class_weights.sum() * NUM_CLASSES

print("\nğŸ“Š Class distribution in training set:")
for i, (class_name, count) in enumerate(zip(class_names, class_counts)):
    weight = class_weights[i]
    print(f"   Class {i + 1}: {class_name.replace('_-_', ' - ')}")
    print(f"      Images: {int(count):,}, Weight: {weight:.4f}")

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ°
sample_weights = [class_weights[label] for _, label in train_dataset]

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ğ°
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataLoader
print("\nâš¡ Creating data loaders...")
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    sampler=sampler,  # Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹
    num_workers=NUM_WORKERS,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

# === Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞŸĞ Ğ•Ğ”ĞĞ‘Ğ£Ğ§Ğ•ĞĞĞĞ™ RESNET50 Ğ˜Ğ— Ğ›ĞĞšĞĞ›Ğ¬ĞĞĞ“Ğ Ğ¤ĞĞ™Ğ›Ğ ===
print("\n" + "=" * 60)
print("ğŸ§  LOADING PRE-TRAINED RESNET50 FROM LOCAL FILE")
print("=" * 60)

# ĞŸÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²ĞµÑĞ¾Ğ²
possible_weights_paths = [
    "models/resnet50_weights/resnet50-11ad3fa6.pth",
    "models/resnet50-11ad3fa6.pth",
    "resnet50_weights/resnet50-11ad3fa6.pth",
    "resnet50-11ad3fa6.pth",
    os.path.join(os.path.expanduser("~"), ".cache", "torch", "hub", "checkpoints", "resnet50-11ad3fa6.pth")
]

weights_path = None
for path in possible_weights_paths:
    if os.path.exists(path):
        weights_path = path
        file_size = os.path.getsize(path) / (1024 * 1024)  # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ² MB
        print(f"âœ… Found weights file at: {path}")
        print(f"   File size: {file_size:.2f} MB (expected: ~97.8 MB)")
        if abs(file_size - 97.8) > 5.0:  # Ğ”Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ 5MB
            print(f"âš ï¸  Warning: File size seems incorrect. Expected ~97.8 MB, got {file_size:.2f} MB")
        break

if weights_path is None:
    print("âŒ ResNet50 weights file not found in any of the expected locations!")
    print("\nğŸ’¡ EXPECTED LOCATIONS:")
    print("1. models/resnet50_weights/resnet50-11ad3fa6.pth (RECOMMENDED)")
    print("2. models/resnet50-11ad3fa6.pth")
    print("3. resnet50_weights/resnet50-11ad3fa6.pth")
    print("4. resnet50-11ad3fa6.pth")

    print("\nğŸ”§ SETUP INSTRUCTIONS:")
    print("1. Download the weights file from:")
    print("   https://download.pytorch.org/models/resnet50-11ad3fa6.pth")
    print("2. Create folder structure:")
    print("   mkdir models")
    print("   mkdir models/resnet50_weights")
    print("3. Place the downloaded file in:")
    print("   models/resnet50_weights/resnet50-11ad3fa6.pth")

    exit()

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ResNet50
print("\nğŸ”„ Creating empty ResNet50 model...")
model = torchvision.models.resnet50(weights=None)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ²ĞµÑĞ¾Ğ²
try:
    print(f"ğŸ”„ Loading weights from: {weights_path}")
    state_dict = torch.load(weights_path, map_location=device)
    model.load_state_dict(state_dict)
    print("âœ… Weights loaded successfully!")
except RuntimeError as e:
    print(f"âŒ Error loading weights: {str(e)}")
    print("\nğŸ”§ TROUBLESHOOTING:")
    print("1. Verify the file is not corrupted (should be ~97.8 MB)")
    print("2. Check if the file was downloaded completely")
    print("3. Try downloading again from the official URL")
    print("4. Ensure you have enough disk space")
    exit()
except Exception as e:
    print(f"âŒ Unexpected error: {str(e)}")
    exit()

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ²
print("\nğŸ” Verifying loaded weights...")
model = model.to(device)
model.eval()
with torch.no_grad():
    sample_input = torch.randn(1, 3, 224, 224).to(device)
    sample_output = model(sample_input)
print(f"âœ… Model forward pass successful! Output shape: {sample_output.shape}")

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° Ğ²ÑĞµÑ… ÑĞ»Ğ¾ĞµĞ² ĞºÑ€Ğ¾Ğ¼Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ…
print("ğŸ”’ Freezing base layers...")
for param in model.parameters():
    param.requires_grad = False

# Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸
print(f"ğŸ”§ Modifying classifier for {NUM_CLASSES} classes...")
num_ftrs = model.fc.in_features
model.fc = torch.nn.Sequential(
    torch.nn.Linear(num_ftrs, 1024),
    torch.nn.ReLU(),
    torch.nn.Dropout(0.5),
    torch.nn.Linear(1024, 512),
    torch.nn.ReLU(),
    torch.nn.Dropout(0.3),
    torch.nn.Linear(512, NUM_CLASSES)
)

# ĞŸĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° GPU
model = model.to(device)
model.train()  # ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
print("âœ… Model moved to GPU and ready for training!")

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹
class_weights_tensor = class_weights.to(device)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)
optimizer = torch.optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.1, patience=3
)

print(f"\nâš™ï¸ Training configuration:")
print(f"   Batch size: {BATCH_SIZE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   Learning rate: {LEARNING_RATE}")
print(f"   Optimizer: Adam (only for classifier)")
print(f"   Scheduler: ReduceLROnPlateau")
print(f"   Early stopping patience: {PATIENCE} epochs")
print(f"   Class balancing: Enabled")
print(f"   Advanced augmentation: Enabled")


# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾Ñ…Ğ¸
def train_one_epoch(epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]")

    for batch_idx, (inputs, labels) in enumerate(progress_bar):
        inputs, labels = inputs.to(device), labels.to(device)

        # ĞĞ±Ğ½ÑƒĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        loss.backward()
        optimizer.step()

        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ°
        accuracy = 100. * correct / total
        progress_bar.set_postfix({
            'Loss': f"{running_loss / (batch_idx + 1):.4f}",
            'Acc': f"{accuracy:.2f}%"
        })

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc


# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
def validate():
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc="Validating", leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_loss = running_loss / len(val_loader)
    val_acc = 100. * correct / total
    return val_loss, val_acc


# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ²
def unfreeze_layers(epoch):
    """ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸"""
    if epoch == 10:  # ĞŸĞ¾ÑĞ»Ğµ 10 ÑĞ¿Ğ¾Ñ…
        print("\nğŸ”“ Unfreezing layer4 (last convolutional block)...")
        for name, param in model.named_parameters():
            if "layer4" in name:
                param.requires_grad = True

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
        optimizer.add_param_group(
            {'params': [p for n, p in model.named_parameters() if "layer4" in n and p.requires_grad],
             'lr': LEARNING_RATE * 0.1})

    if epoch == 25:  # ĞŸĞ¾ÑĞ»Ğµ 25 ÑĞ¿Ğ¾Ñ…
        print("\nğŸ”“ Unfreezing layer3...")
        for name, param in model.named_parameters():
            if "layer3" in name and param.requires_grad == False:
                param.requires_grad = True

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
        optimizer.add_param_group(
            {'params': [p for n, p in model.named_parameters() if "layer3" in n and p.requires_grad],
             'lr': LEARNING_RATE * 0.05})


# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
if __name__ == '__main__':
    print(f"\nğŸ”¥ Starting training for {NUM_EPOCHS} epochs...")
    best_val_acc = 0
    patience_counter = 0
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    training_start_time = time.time()

    for epoch in range(NUM_EPOCHS):
        # ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ²
        unfreeze_layers(epoch)

        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        train_loss, train_acc = train_one_epoch(epoch)

        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
        val_loss, val_acc = validate()

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ scheduler
        scheduler.step(val_acc)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            print(f"ğŸ† New best model saved! Validation accuracy: {val_acc:.2f}%")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"â³ No improvement for {patience_counter} epochs")

        # Early stopping
        if patience_counter >= PATIENCE:
            print(f"ğŸš¨ Early stopping triggered after {epoch + 1} epochs!")
            break

        print(f"\nğŸ“Š Epoch {epoch + 1}/{NUM_EPOCHS} results:")
        print(f"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"   Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}%")
        print(f"   Best Val Acc so far: {best_val_acc:.2f}%")
        print(f"   Current learning rate: {optimizer.param_groups[0]['lr']:.6f}")

    training_end_time = time.time()
    print(f"\nâœ… Training completed in {training_end_time - training_start_time:.2f} seconds!")

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    print("\nğŸ§ª Loading best model for testing...")
    model.load_state_dict(torch.load(BEST_MODEL_PATH))
    model.eval()

    # Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    print("ğŸ” Testing on test set...")
    test_correct = 0
    test_total = 0
    test_predictions = []
    test_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Testing"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            test_predictions.extend(predicted.cpu().numpy())
            test_labels.extend(labels.cpu().numpy())

    test_acc = 100. * test_correct / test_total
    print(f"\nğŸ¯ Final Test Accuracy: {test_acc:.2f}%")

    # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    print("\nğŸ“Š Generating training plots...")

    plt.figure(figsize=(12, 5))

    # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸
    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label='Train Accuracy')
    plt.plot(val_accs, label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, 'training_history_final.png'))
    print(f"âœ… Training history saved to: {os.path.join(RESULTS_DIR, 'training_history_final.png')}")

    # ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (confusion matrix)
    print("\nğŸ“Š Generating confusion matrix...")
    cm = confusion_matrix(test_labels, test_predictions)
    plt.figure(figsize=(15, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, 'confusion_matrix_final.png'))
    print(f"âœ… Confusion matrix saved to: {os.path.join(RESULTS_DIR, 'confusion_matrix_final.png')}")

    # ĞÑ‚Ñ‡ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
    print("\nğŸ“‹ Classification Report:")
    report = classification_report(test_labels, test_predictions, target_names=class_names)
    print(report)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
    with open(os.path.join(RESULTS_DIR, 'classification_report_final.txt'), 'w') as f:
        f.write(report)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    print(f"\nğŸ’¾ Final model saved to: {MODEL_SAVE_PATH}")


    # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…
    def predict_image(image_path):
        """ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑ‚ĞµĞ½Ğ¸Ñ"""

        try:
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
            img = Image.open(image_path).convert('RGB')
            transform = val_test_transforms
            img_tensor = transform(img).unsqueeze(0).to(device)

            # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
            with torch.no_grad():
                output = model(img_tensor)
                probabilities = torch.nn.functional.softmax(output, dim=1)
                confidence, predicted_idx = torch.max(probabilities, 1)

            # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ°
            predicted_class = class_names[predicted_idx.item()]
            confidence_value = confidence.item() * 100

            return {
                'class': predicted_class,
                'confidence': confidence_value,
                'probabilities': probabilities.cpu().numpy()[0]
            }

        except Exception as e:
            return {'error': str(e)}


    # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
    print("\nğŸ” Testing prediction function with real dataset images...")
    sample_images = []

    # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ 1 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ° Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    for class_name in class_names[:5]:  # Ğ¢ĞµÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… 5 ĞºĞ»Ğ°ÑÑĞ¾Ğ²
        class_path = os.path.join(DATASET_PATH, class_name)
        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        if image_files:
            sample_images.append(os.path.join(class_path, random.choice(image_files)))

    for img_path in sample_images:
        print(f"\nğŸ“¸ Testing image: {os.path.basename(img_path)}")
        result = predict_image(img_path)
        if 'error' not in result:
            print(f"âœ… Prediction:")
            print(f"   Class: {result['class'].replace('_-_', ' - ')}")
            print(f"   Confidence: {result['confidence']:.2f}%")
        else:
            print(f"âŒ Prediction error: {result['error']}")

    print(f"\nğŸ‰ SUCCESS! Training completed successfully!")
    print(f"ğŸ“ Best model saved to: {BEST_MODEL_PATH}")
    print(f"ğŸ“Š Results saved to: {RESULTS_DIR}/")
    print(f"\nğŸ¯ Final Result: Test Accuracy = {test_acc:.2f}%")
    if test_acc >= 85.0:
        print("âœ… EXCELLENT! Model accuracy exceeds 85% target!")
    elif test_acc >= 80.0:
        print("âœ… GOOD! Model accuracy meets minimum requirement for production.")
    else:
        print("âš ï¸  Model accuracy is below 80% target. Consider further improvements.")

    print("\nğŸš€ Next steps - creating Flask web application:")
    print("1. Create app/__init__.py for Flask application factory")
    print("2. Create app/routes.py for image upload and prediction endpoints")
    print("3. Create templates for user interface")
    print("4. Set up Docker containerization")